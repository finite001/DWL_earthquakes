{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "comprehensive-anderson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-deposit",
   "metadata": {},
   "source": [
    "### Connection to AWS DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "brave-steel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the connection details for the rds db from .env file\n",
    "config = dotenv_values(\".env\")  \n",
    "HOST_RDS = config['HOST_RDS']\n",
    "DBNAME_RDS = config['DBNAME_RDS']\n",
    "USER_RDS = config['USER_RDS']\n",
    "PASSWORD_RDS = config['PASSWORD_RDS']\n",
    "\n",
    "\n",
    "try: \n",
    "    conn = psycopg2.connect(host=HOST_RDS, dbname=DBNAME_RDS, user=USER_RDS, password=PASSWORD_RDS)\n",
    "except psycopg2.Error as e: \n",
    "    print(\"Error: Could not make connection to the Postgres database\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "informational-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    cur = conn.cursor()\n",
    "except psycopg2.Error as e: \n",
    "    print(\"Error: Could not get curser to the Database\")\n",
    "    print(e)\n",
    "    \n",
    "# Auto commit is very important\n",
    "conn.set_session(autocommit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "desirable-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"DROP TABLE IF EXISTS tweets;\")\n",
    "cur.execute(\"DROP TABLE IF EXISTS tweets_user;\")\n",
    "\n",
    "# cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS tweets (\n",
    "#             text text, \n",
    "#             author_id bigint,\n",
    "#             id bigint,\n",
    "#             created_at text);\"\"\")\n",
    "\n",
    "\n",
    "# cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS tweets_user (\n",
    "#             id bigint, \n",
    "#             username text, \n",
    "#             name text, \n",
    "#             location text);\"\"\")\n",
    "\n",
    "table_name_t = 'tweets'\n",
    "table_name_u = 'tweets_user'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-favorite",
   "metadata": {},
   "source": [
    "### Accessing Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "maritime-royal",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEARER_TOKEN = config['BEARER_TOKEN']\n",
    "\n",
    "# path where twitter files will be stored\n",
    "path = \"Data/twitter/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "inappropriate-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_QUERY = \"-from:quakeupdates -from:jojo2727 -from:MonitorSismico -from:MyComicalLife -from:news_sokuho_bot -from:DiariosRobot -from:EN_NERV -from:GDACS -from:earthquake_jp -from:EQAlerts -from:j1_quake -from:iSachinSrivstva -from:VolcanoEWS -from:ChileAlertaApp -from:earthb0t -from:sexy_vegetables -from:zishin3255 -from:everyEarthquake -from:MapQuake -from:swap_bot_bash -from:eq_map -from:eq_map_es -from:eq_map_ww -from:SEISMOinfo -from:VegaBajaWx -from:WatchOurCity -from:Keith_Event -from:SismoDetector -from:cvb_223 -from:ExBulletinUk -from:EMSC -from:StoixeioJewelry -from:megamodo -from:earthquakevt -from:QuakeBotter -from:twtaka_jp -from:EarthquakeTw -from:ENSO1998 -from:eq_map_ww2 -from:eq_map_es2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "wooden-gateway",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = '2018-10-01T00:00:00.000Z'\n",
    "end_time = '2018-12-31T23:59:59.000Z'\n",
    "query = \"earthquake -minor, -is:reply -is:retweet {0}\".format(FILTER_QUERY)\n",
    "max_results = \"500\"\n",
    "tweet_fields = \"created_at,author_id\"\n",
    "user_fields = 'username,location'\n",
    "expansions = 'author_id'\n",
    "query_params = {'query': query, 'tweet.fields': tweet_fields, 'user.fields': user_fields, \\\n",
    "                'start_time': start_time, 'end_time': end_time, 'max_results': max_results, \\\n",
    "                'expansions': expansions}\n",
    "url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "headers = {\"Authorization\": \"Bearer \" + BEARER_TOKEN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "proof-jewel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019-07-01T00:00:00.000Z'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "painful-happiness",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-782755e3bfa5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mnext_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_response\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'meta'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'next_token'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m           \u001b[1;31m#  logging.info(\"Fetching next few tweets, next_token: \", query_params['next_token'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m'next_token'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mquery_params\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "users = []\n",
    "while True:\n",
    "    # get results according to url and query\n",
    "    response = requests.request(\"GET\", url, headers=headers, params=query_params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "\n",
    "    # combine data to one\n",
    "    json_response = response.json()\n",
    "    if 'data' in json_response:\n",
    "        tweets = tweets + json_response['data']\n",
    "        users = users + json_response['includes']['users']\n",
    "\n",
    "    # check if more data available, if yes continue process\n",
    "    if 'meta' in json_response:\n",
    "        if 'next_token' in json_response['meta']:\n",
    "            query_params['next_token'] = json_response['meta']['next_token']\n",
    "            next_token = json_response['meta']['next_token']\n",
    "          #  logging.info(\"Fetching next few tweets, next_token: \", query_params['next_token'])\n",
    "            time.sleep(3)\n",
    "        else:\n",
    "            if 'next_token' in query_params:\n",
    "                del query_params['next_token']\n",
    "            break\n",
    "    else:\n",
    "        if 'next_token' in query_params:\n",
    "            del query_params['next_token']\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "drawn-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define query parameters \n",
    "query = \"earthquake -minor, -is:reply -is:retweet \" + filter_names_query\n",
    "start_time = \"2021-11-15T00:00:00.000Z\"\n",
    "end_time = \"2021-11-15T12:49:59.000Z\"\n",
    "max_results = \"500\"\n",
    "tweet_fields = \"created_at,author_id\" \n",
    "user_fields = 'username,location' \n",
    "expansions = 'author_id'\n",
    "\n",
    "# put query parameters in a dict\n",
    "query_params = {'query': query,'tweet.fields': tweet_fields, 'user.fields': user_fields,  \\\n",
    "                'start_time': start_time, 'end_time': end_time, 'max_results': max_results,\\\n",
    "                'expansions': expansions}\n",
    "\n",
    "url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "\n",
    "# define headers for authorization\n",
    "headers = {\"Authorization\": \"Bearer \" + BEARER_TOKEN}\n",
    "\n",
    "data_t = []\n",
    "data_u = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "international-textbook",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching next few tweets, next_token:  b26v89c19zqg8o3fpdy5k84dkewj58oiwmlbobujncjgd\n",
      "Fetching next few tweets, next_token:  b26v89c19zqg8o3fpdy5k836d6s9fhzf1a00n9yii5ke5\n",
      "Fetching next few tweets, next_token:  b26v89c19zqg8o3fpdy5k60960edlaspc15pz4ypcrfjx\n",
      "Fetching next few tweets, next_token:  b26v89c19zqg8o3fpdy5k5zcsrjw42l0mlk34rs6cwrjx\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # get results according to url and query\n",
    "    response = None\n",
    "    response = requests.request(\"GET\", url, headers=headers, params=query_params)\n",
    "    if response.status_code != 200:\n",
    "         raise Exception(response.status_code, response.text)\n",
    "\n",
    "    # combine data to one\n",
    "    json_response = response.json()\n",
    "    if 'data' in json_response:\n",
    "        data_t = data_t + json_response['data']\n",
    "        data_u = data_u + json_response['includes']['users']\n",
    "\n",
    "    # check if more data available, if yes continue process\n",
    "    if 'meta' in json_response:\n",
    "        if 'next_token' in json_response['meta']:\n",
    "            query_params['next_token'] = json_response['meta']['next_token']\n",
    "            next_token = json_response['meta']['next_token']\n",
    "            print(\"Fetching next few tweets, next_token: \",query_params['next_token'])\n",
    "            time.sleep(3)\n",
    "        else:\n",
    "            if 'next_token' in query_params:\n",
    "                del query_params['next_token']\n",
    "            break\n",
    "    else:\n",
    "        if 'next_token' in query_params:\n",
    "            del query_params['next_token']\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "domestic-moisture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4063363075256348\n"
     ]
    }
   ],
   "source": [
    "# add location to all users, empty string if element does not exist (to insert data into table)\n",
    "for item in data_u:\n",
    "    if 'location' in item:\n",
    "        pass\n",
    "    else:\n",
    "        item['location'] = \"\"\n",
    "        \n",
    "# create iterators\n",
    "iter_tweets = iter(data_t)\n",
    "iter_users = iter(data_u)\n",
    "\n",
    "# insert tweets\n",
    "psycopg2.extras.execute_batch(cur, \"\"\"INSERT INTO tweets VALUES(\n",
    "%(text)s,\n",
    "%(author_id)s,\n",
    "%(id)s,\n",
    "%(created_at)s\n",
    ");\"\"\",iter_tweets)\n",
    "\n",
    "# insert users\n",
    "psycopg2.extras.execute_batch(cur, \"\"\"INSERT INTO tweets_user VALUES(\n",
    "%(id)s,\n",
    "%(username)s,\n",
    "%(name)s,\n",
    "%(location)s\n",
    ");\"\"\",iter_users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "positive-saudi",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    cur.execute(\"INSERT INTO twitter_eq (col1, col2, col3) \\\n",
    "                 VALUES (%s, %s, %s)\", \\\n",
    "                 (10, 20, 1970), \\\n",
    "                ()\n",
    "                \n",
    "               )\n",
    "except psycopg2.Error as e: \n",
    "    print(\"Error: Inserting Rows\")\n",
    "    print (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "supreme-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    cur.execute(\"SELECT * FROM twitter_eq;\")\n",
    "except psycopg2.Error as e: \n",
    "    print(\"Error: select *\")\n",
    "    print (e)\n",
    "\n",
    "row = cur.fetchone()\n",
    "while row:\n",
    "   print(row)\n",
    "   row = cur.fetchone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "entitled-identification",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS timeframes2 (\n",
    "            id int,\n",
    "            startdate text, \n",
    "            enddate text,\n",
    "            requested boolean);\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "recognized-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1, '2019-10-01T00:00:00.000Z', '2019-12-31T23:59:59.000Z', False), \\\n",
    "        (2, '2019-07-01T00:00:00.000Z', '2019-09-30T23:59:59.000Z', False), \\\n",
    "        (3, '2019-04-01T00:00:00.000Z', '2019-06-30T23:59:59.000Z', False), \\\n",
    "        (4, '2019-01-01T00:00:00.000Z', '2019-03-31T23:59:59.000Z', False), \\\n",
    "        (5, '2018-10-01T00:00:00.000Z', '2018-12-31T23:59:59.000Z', False), \\\n",
    "        (6, '2018-07-01T00:00:00.000Z', '2018-09-30T23:59:59.000Z', False), \\\n",
    "        (7, '2018-04-01T00:00:00.000Z', '2018-06-30T23:59:59.000Z', False), \\\n",
    "        (8, '2018-01-01T00:00:00.000Z', '2018-03-31T23:59:59.000Z', False), \\\n",
    "        (9, '2017-10-01T00:00:00.000Z', '2017-12-31T23:59:59.000Z', False), \\\n",
    "        (10, '2017-07-01T00:00:00.000Z', '2017-09-30T23:59:59.000Z', False), \\\n",
    "        (11, '2017-04-01T00:00:00.000Z', '2017-06-30T23:59:59.000Z', False), \\\n",
    "        (12, '2017-01-01T00:00:00.000Z', '2017-03-31T23:59:59.000Z', False), \\\n",
    "        (13, '2016-10-01T00:00:00.000Z', '2016-12-31T23:59:59.000Z', False), \\\n",
    "        (14, '2016-07-01T00:00:00.000Z', '2016-09-30T23:59:59.000Z', False), \\\n",
    "        (15, '2016-04-01T00:00:00.000Z', '2016-06-30T23:59:59.000Z', False), \\\n",
    "        (16, '2016-01-01T00:00:00.000Z', '2016-03-31T23:59:59.000Z', False), \\\n",
    "        (17, '2015-10-01T00:00:00.000Z', '2015-12-31T23:59:59.000Z', False), \\\n",
    "        (18, '2015-07-01T00:00:00.000Z', '2015-09-30T23:59:59.000Z', False), \\\n",
    "        (19, '2015-04-01T00:00:00.000Z', '2015-06-30T23:59:59.000Z', False), \\\n",
    "        (20, '2015-01-01T00:00:00.000Z', '2015-03-31T23:59:59.000Z', False), \\\n",
    "       ]\n",
    "\n",
    "data2 = [(11, '2017-04-01T00:00:00.000Z', '2017-06-30T23:59:59.000Z', False), \\\n",
    "        (12, '2017-01-01T00:00:00.000Z', '2017-03-31T23:59:59.000Z', False), \\\n",
    "        (13, '2016-10-01T00:00:00.000Z', '2016-12-31T23:59:59.000Z', False), \\\n",
    "        (14, '2016-07-01T00:00:00.000Z', '2016-09-30T23:59:59.000Z', False), \\\n",
    "        (15, '2016-04-01T00:00:00.000Z', '2016-06-30T23:59:59.000Z', False), \\\n",
    "        (16, '2016-01-01T00:00:00.000Z', '2016-03-31T23:59:59.000Z', False), \\\n",
    "        (17, '2015-10-01T00:00:00.000Z', '2015-12-31T23:59:59.000Z', False), \\\n",
    "        (18, '2015-07-01T00:00:00.000Z', '2015-09-30T23:59:59.000Z', False), \\\n",
    "        (19, '2015-04-01T00:00:00.000Z', '2015-06-30T23:59:59.000Z', False), \\\n",
    "        (20, '2015-01-01T00:00:00.000Z', '2015-03-31T23:59:59.000Z', False), \\\n",
    "        (21, '2014-10-01T00:00:00.000Z', '2014-12-31T23:59:59.000Z', False), \\\n",
    "        (22, '2014-07-01T00:00:00.000Z', '2014-09-30T23:59:59.000Z', False), \\\n",
    "        (23, '2014-04-01T00:00:00.000Z', '2014-06-30T23:59:59.000Z', False), \\\n",
    "        (24, '2014-01-01T00:00:00.000Z', '2014-03-31T23:59:59.000Z', False), \\\n",
    "        (25, '2013-10-01T00:00:00.000Z', '2013-12-31T23:59:59.000Z', False), \\\n",
    "        (26, '2013-07-01T00:00:00.000Z', '2013-09-30T23:59:59.000Z', False), \\\n",
    "        (27, '2013-04-01T00:00:00.000Z', '2013-06-30T23:59:59.000Z', False), \\\n",
    "        (28, '2013-01-01T00:00:00.000Z', '2013-03-31T23:59:59.000Z', False), \\\n",
    "        (29, '2012-10-01T00:00:00.000Z', '2012-12-31T23:59:59.000Z', False), \\\n",
    "        (30, '2012-07-01T00:00:00.000Z', '2012-09-30T23:59:59.000Z', False), \\\n",
    "        (31, '2012-04-01T00:00:00.000Z', '2012-06-30T23:59:59.000Z', False), \\\n",
    "        (32, '2012-01-01T00:00:00.000Z', '2012-03-31T23:59:59.000Z', False), \\\n",
    "        (33, '2011-10-01T00:00:00.000Z', '2011-12-31T23:59:59.000Z', False), \\\n",
    "        (34, '2011-07-01T00:00:00.000Z', '2011-09-30T23:59:59.000Z', False), \\\n",
    "        (35, '2011-04-01T00:00:00.000Z', '2011-06-30T23:59:59.000Z', False), \\\n",
    "        (36, '2011-01-01T00:00:00.000Z', '2011-03-31T23:59:59.000Z', False), \\\n",
    "        (37, '2010-10-01T00:00:00.000Z', '2010-12-31T23:59:59.000Z', False), \\\n",
    "        (38, '2010-07-01T00:00:00.000Z', '2010-09-30T23:59:59.000Z', False), \\\n",
    "        (39, '2010-04-01T00:00:00.000Z', '2010-06-30T23:59:59.000Z', False), \\\n",
    "        (40, '2010-01-01T00:00:00.000Z', '2010-03-31T23:59:59.000Z', False), \\\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "decreased-occasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    cur.executemany(\"\"\"INSERT INTO timeframes2 (id, startdate, enddate, requested) \n",
    "                 VALUES (%s, %s, %s, %s)\"\"\", \\\n",
    "                 data2)\n",
    "except psycopg2.Error as e: \n",
    "    print(\"Error: Inserting Rows\")\n",
    "    print (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "assured-blake",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
