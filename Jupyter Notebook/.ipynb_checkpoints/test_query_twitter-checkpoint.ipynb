{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "canadian-plastic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the requests library\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mathematical-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values(\".env\")  \n",
    "BEARER_TOKEN = config['BEARER_TOKEN']\n",
    "# path where twitter files will be stored\n",
    "path = \"Data/twitter/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "promotional-constitutional",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define query parameters \n",
    "query = \"earthquake -minor, -is:reply -is:retweet\" #-from:username / -is:retweet\n",
    "start_time = \"2021-10-17T00:00:00.000Z\"\n",
    "end_time = \"2021-10-17T23:59:59.000Z\"\n",
    "max_results = \"500\"\n",
    "tweet_fields = \"created_at,author_id,geo\" # geo produes unreliable results. Kept anyway to simulate dirty data\n",
    "user_fields = 'username,location' # location is same as geo\n",
    "file_counter = 0\n",
    "expansions = 'author_id'\n",
    "\n",
    "\n",
    "\n",
    "# put query parameters in a list\n",
    "query_params = {'query': query,'tweet.fields': tweet_fields, 'user.fields': user_fields,  \\\n",
    "                'start_time': start_time, 'end_time': end_time, 'max_results': max_results,\\\n",
    "                'expansions': expansions}\n",
    "\n",
    "url = \"https://api.twitter.com/2/tweets/search/all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "orange-catering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to fetch data\n",
      "Fetching next few tweets, next_token:  b26v89c19zqg8o3fpds9t9fy2y1081q5ndl16bsdgsjnh\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ac326d00b314>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mnext_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_response\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'meta'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'next_token'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fetching next few tweets, next_token: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mquery_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'next_token'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mfile_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define headers for authorization\n",
    "headers = {\"Authorization\": \"Bearer \" + BEARER_TOKEN}\n",
    "\n",
    "print(\"Starting to fetch data\")\n",
    "\n",
    "\n",
    "while True:\n",
    "    # get results according to url and query\n",
    "    response = None\n",
    "    response = requests.request(\"GET\", url, headers=headers, params=query_params)\n",
    "              \n",
    "    if response.status_code != 200:\n",
    "         raise Exception(response.status_code, response.text)\n",
    "    \n",
    "    # create json out of result\n",
    "    json_response = response.json()\n",
    "    \n",
    "    # write data into txt\n",
    "    with open(path + \"twitter_file_\" + str(file_counter) + \".txt\",\n",
    "              \"w\") as outfile:\n",
    "        outfile.write(json.dumps(json_response, indent=4))\n",
    "\n",
    "    # check if more data available, if yes continue process\n",
    "    file_counter += 1\n",
    "    if 'meta' in json_response:\n",
    "        if 'next_token' in json_response['meta']:\n",
    "            query_params['next_token'] = json_response['meta']['next_token']\n",
    "            next_token = json_response['meta']['next_token']\n",
    "            print(\"Fetching next few tweets, next_token: \",query_params['next_token'])\n",
    "            time.sleep(4)\n",
    "        else:\n",
    "            file_counter = 0\n",
    "            del query_params['next_token']\n",
    "            break\n",
    "    else:\n",
    "        file_counter = 0\n",
    "        del query_params['next_token']\n",
    "        break\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "convertible-entertainment",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# create list of all file names with path\n",
    "file_list = glob.glob(os.path.join(os.getcwd(), \"Data\\\\twitter\", \"*.txt\"))\n",
    "\n",
    "tweets = []\n",
    "for file in file_list:\n",
    "    with open(file) as f:\n",
    "        tweets.append(json.load(f))\n",
    "        \n",
    "# create df out of the data\n",
    "df_tweets = pd.DataFrame()\n",
    "df_users = pd.DataFrame()\n",
    "for tweet in tweets:\n",
    "    df_tweets=df_tweets.append(pd.json_normalize(tweet['data']))\n",
    "    df_users=df_users.append(pd.json_normalize(tweet['includes']['users']))\n",
    "    \n",
    "\n",
    "df_users = df_users.rename(columns={'id':'author_id'})\n",
    "df = pd.merge(df_tweets, df_users, on='author_id').drop_duplicates(subset='id').reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "reduced-ozone",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>id</th>\n",
       "      <th>geo.place_id</th>\n",
       "      <th>geo.coordinates.type</th>\n",
       "      <th>geo.coordinates.coordinates</th>\n",
       "      <th>withheld.copyright</th>\n",
       "      <th>withheld.country_codes_x</th>\n",
       "      <th>username</th>\n",
       "      <th>name</th>\n",
       "      <th>location</th>\n",
       "      <th>withheld.country_codes_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-10-17T23:59:54.000Z</td>\n",
       "      <td>#Earthquake M4.7 CANARY ISLANDS, SPAIN REGION ...</td>\n",
       "      <td>809537352909656064</td>\n",
       "      <td>1449887926985363456</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EQAlerts</td>\n",
       "      <td>Earthquake Monitor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-10-17T23:58:02.000Z</td>\n",
       "      <td>Moderate magnitude 4.3 #earthquake 24 km north...</td>\n",
       "      <td>809537352909656064</td>\n",
       "      <td>1449887457193963525</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EQAlerts</td>\n",
       "      <td>Earthquake Monitor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-10-17T23:35:21.000Z</td>\n",
       "      <td>#Earthquake M4.4 Mexico: 82 Km Al Oeste De Chi...</td>\n",
       "      <td>809537352909656064</td>\n",
       "      <td>1449881750532939776</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EQAlerts</td>\n",
       "      <td>Earthquake Monitor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-10-17T23:12:10.000Z</td>\n",
       "      <td>#Earthquake M4.0 MINAHASA, SULAWESI, INDONESIA...</td>\n",
       "      <td>809537352909656064</td>\n",
       "      <td>1449875917761884164</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EQAlerts</td>\n",
       "      <td>Earthquake Monitor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-10-17T23:02:48.000Z</td>\n",
       "      <td>#Earthquake M3.6 ISLAND OF HAWAII, HAWAII 4min...</td>\n",
       "      <td>809537352909656064</td>\n",
       "      <td>1449873557748596739</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EQAlerts</td>\n",
       "      <td>Earthquake Monitor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4953</th>\n",
       "      <td>2021-10-17T00:11:48.000Z</td>\n",
       "      <td>Earthquake kills 3 in Bali, Indonesia, destroy...</td>\n",
       "      <td>2282930023</td>\n",
       "      <td>1449528535132119043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7SealsOfTheEnd</td>\n",
       "      <td>7 Seals Of The End</td>\n",
       "      <td>News &amp; Bible verses USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4954</th>\n",
       "      <td>2021-10-17T00:08:42.000Z</td>\n",
       "      <td>4.8 Magnitude Earthquake Strikes Indonesia‚Äôs B...</td>\n",
       "      <td>367694191</td>\n",
       "      <td>1449527755733753857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ANC_News2</td>\n",
       "      <td>Sanjeev Dev Malik</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4955</th>\n",
       "      <td>2021-10-17T00:08:00.000Z</td>\n",
       "      <td>Moderate earthquake rocks Bali, killing at lea...</td>\n",
       "      <td>1313407652202975232</td>\n",
       "      <td>1449527580844077059</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LindaMi14118735</td>\n",
       "      <td>Linda Miranda</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4956</th>\n",
       "      <td>2021-10-17T00:05:38.000Z</td>\n",
       "      <td>A terrible #earthquake hits #Indonesia, causin...</td>\n",
       "      <td>757114028489580544</td>\n",
       "      <td>1449526984086855681</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>News_Disaster1</td>\n",
       "      <td>ŸÜŸèÿ∞ÿ± ÿßŸÑÿπÿ∞ÿßÿ® nibiru</td>\n",
       "      <td>ŸÖŸÜÿ™ÿØŸäÿßÿ™ ÿßŸÑÿ®ÿ¥ÿ±Ÿâ ÿßŸÑÿ•ÿ≥ŸÑÿßŸÖŸäŸá</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4957</th>\n",
       "      <td>2021-10-17T00:02:19.000Z</td>\n",
       "      <td>The early 20th century Gaddi Baithak stands ta...</td>\n",
       "      <td>19006707</td>\n",
       "      <td>1449526149948784645</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ECAatState</td>\n",
       "      <td>Exchange Programs</td>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4958 rows √ó 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    created_at  \\\n",
       "0     2021-10-17T23:59:54.000Z   \n",
       "1     2021-10-17T23:58:02.000Z   \n",
       "2     2021-10-17T23:35:21.000Z   \n",
       "3     2021-10-17T23:12:10.000Z   \n",
       "4     2021-10-17T23:02:48.000Z   \n",
       "...                        ...   \n",
       "4953  2021-10-17T00:11:48.000Z   \n",
       "4954  2021-10-17T00:08:42.000Z   \n",
       "4955  2021-10-17T00:08:00.000Z   \n",
       "4956  2021-10-17T00:05:38.000Z   \n",
       "4957  2021-10-17T00:02:19.000Z   \n",
       "\n",
       "                                                   text            author_id  \\\n",
       "0     #Earthquake M4.7 CANARY ISLANDS, SPAIN REGION ...   809537352909656064   \n",
       "1     Moderate magnitude 4.3 #earthquake 24 km north...   809537352909656064   \n",
       "2     #Earthquake M4.4 Mexico: 82 Km Al Oeste De Chi...   809537352909656064   \n",
       "3     #Earthquake M4.0 MINAHASA, SULAWESI, INDONESIA...   809537352909656064   \n",
       "4     #Earthquake M3.6 ISLAND OF HAWAII, HAWAII 4min...   809537352909656064   \n",
       "...                                                 ...                  ...   \n",
       "4953  Earthquake kills 3 in Bali, Indonesia, destroy...           2282930023   \n",
       "4954  4.8 Magnitude Earthquake Strikes Indonesia‚Äôs B...            367694191   \n",
       "4955  Moderate earthquake rocks Bali, killing at lea...  1313407652202975232   \n",
       "4956  A terrible #earthquake hits #Indonesia, causin...   757114028489580544   \n",
       "4957  The early 20th century Gaddi Baithak stands ta...             19006707   \n",
       "\n",
       "                       id geo.place_id geo.coordinates.type  \\\n",
       "0     1449887926985363456          NaN                  NaN   \n",
       "1     1449887457193963525          NaN                  NaN   \n",
       "2     1449881750532939776          NaN                  NaN   \n",
       "3     1449875917761884164          NaN                  NaN   \n",
       "4     1449873557748596739          NaN                  NaN   \n",
       "...                   ...          ...                  ...   \n",
       "4953  1449528535132119043          NaN                  NaN   \n",
       "4954  1449527755733753857          NaN                  NaN   \n",
       "4955  1449527580844077059          NaN                  NaN   \n",
       "4956  1449526984086855681          NaN                  NaN   \n",
       "4957  1449526149948784645          NaN                  NaN   \n",
       "\n",
       "     geo.coordinates.coordinates withheld.copyright withheld.country_codes_x  \\\n",
       "0                            NaN                NaN                      NaN   \n",
       "1                            NaN                NaN                      NaN   \n",
       "2                            NaN                NaN                      NaN   \n",
       "3                            NaN                NaN                      NaN   \n",
       "4                            NaN                NaN                      NaN   \n",
       "...                          ...                ...                      ...   \n",
       "4953                         NaN                NaN                      NaN   \n",
       "4954                         NaN                NaN                      NaN   \n",
       "4955                         NaN                NaN                      NaN   \n",
       "4956                         NaN                NaN                      NaN   \n",
       "4957                         NaN                NaN                      NaN   \n",
       "\n",
       "             username                name                   location  \\\n",
       "0            EQAlerts  Earthquake Monitor                        NaN   \n",
       "1            EQAlerts  Earthquake Monitor                        NaN   \n",
       "2            EQAlerts  Earthquake Monitor                        NaN   \n",
       "3            EQAlerts  Earthquake Monitor                        NaN   \n",
       "4            EQAlerts  Earthquake Monitor                        NaN   \n",
       "...               ...                 ...                        ...   \n",
       "4953   7SealsOfTheEnd  7 Seals Of The End    News & Bible verses USA   \n",
       "4954        ANC_News2   Sanjeev Dev Malik                    Gurgaon   \n",
       "4955  LindaMi14118735       Linda Miranda                        NaN   \n",
       "4956   News_Disaster1  ŸÜŸèÿ∞ÿ± ÿßŸÑÿπÿ∞ÿßÿ® nibiru  ŸÖŸÜÿ™ÿØŸäÿßÿ™ ÿßŸÑÿ®ÿ¥ÿ±Ÿâ ÿßŸÑÿ•ÿ≥ŸÑÿßŸÖŸäŸá    \n",
       "4957       ECAatState   Exchange Programs             Washington, DC   \n",
       "\n",
       "     withheld.country_codes_y  \n",
       "0                         NaN  \n",
       "1                         NaN  \n",
       "2                         NaN  \n",
       "3                         NaN  \n",
       "4                         NaN  \n",
       "...                       ...  \n",
       "4953                      NaN  \n",
       "4954                      NaN  \n",
       "4955                      NaN  \n",
       "4956                      NaN  \n",
       "4957                      NaN  \n",
       "\n",
       "[4958 rows x 13 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "retained-string",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create df with usernames and number of postings\n",
    "account_df = df.username.value_counts()\n",
    "lst_a = account_df.index.tolist()\n",
    "lst_v = account_df.tolist()\n",
    "\n",
    "df_a = pd.DataFrame(lst_a).rename(columns={0: 'username'})\n",
    "df_a['value'] = lst_v\n",
    "\n",
    "# filter out accounts with more or equal to 175 postings\n",
    "df_a = df_a.loc[df_a['value'] > 10].sort_values(by=['value'])\n",
    "lst_bots = df_a.username.tolist()\n",
    "\n",
    "# add -from: operator to each entry in list \n",
    "append_str = '-from:'\n",
    "lst_filter = [append_str + sub for sub in lst_bots]\n",
    "\n",
    "# convert list to a string suitable for the query\n",
    "filter_names_query = ' '.join(lst_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "arbitrary-conversion",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-from:quakeupdates -from:jojo2727 -from:MonitorSismico -from:MyComicalLife -from:news_sokuho_bot -from:DiariosRobot -from:EN_NERV -from:GDACS -from:earthquake_jp -from:EQAlerts -from:j1_quake -from:iSachinSrivstva -from:VolcanoEWS -from:ChileAlertaApp -from:earthb0t -from:sexy_vegetables -from:zishin3255 -from:everyEarthquake -from:MapQuake -from:swap_bot_bash -from:eq_map -from:eq_map_es -from:eq_map_ww -from:SEISMOinfo -from:VegaBajaWx -from:WatchOurCity -from:Keith_Event -from:SismoDetector -from:cvb_223 -from:ExBulletinUk -from:EMSC -from:StoixeioJewelry -from:megamodo -from:earthquakevt -from:QuakeBotter -from:twtaka_jp -from:EarthquakeTw -from:ENSO1998 -from:eq_map_ww2 -from:eq_map_es2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_names_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "prescription-diagram",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define query parameters \n",
    "query = \"earthquake -minor, -is:reply -is:retweet \" + filter_names_query\n",
    "start_time = \"2020-12-01T00:00:00.000Z\"\n",
    "end_time = \"2020-12-31T23:59:59.000Z\"\n",
    "max_results = \"500\"\n",
    "tweet_fields = \"created_at,author_id,geo\" # geo produes unreliable results. Kept anyway to simulate dirty data\n",
    "user_fields = 'username,location' # location is same as geo\n",
    "place_country = 'JP'\n",
    "file_counter = 0\n",
    "expansions = 'author_id'\n",
    "\n",
    "\n",
    "\n",
    "# put query parameters in a list\n",
    "query_params = {'query': query,'tweet.fields': tweet_fields, 'user.fields': user_fields,  \\\n",
    "                'start_time': start_time, 'end_time': end_time, 'max_results': max_results,\\\n",
    "                'expansions': expansions}\n",
    "\n",
    "url = \"https://api.twitter.com/2/tweets/search/all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "enabling-economics",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to fetch data\n",
      "Fetching next few tweets, next_token:  b26v89c19zqg8o3foshtdmqzy88mlrbl6lgpsu4iee7p9\n",
      "Fetching next few tweets, next_token:  b26v89c19zqg8o3foshtdmq3mi88heg2rnp6c46o5pqbh\n",
      "Fetching next few tweets, next_token:  b26v89c19zqg8o3foshtdknrsku7ek14fz611dhkge2yl\n",
      "Fetching next few tweets, next_token:  b26v89c19zqg8o3foshtdkngx47trxbyij0i6tm1255kt\n",
      "Fetching next few tweets, next_token:  b26v89c19zqg8o3foshtdkn5ym504fb19hgyxidlrn5a5\n",
      "Fetching next few tweets, next_token:  b26v89c19zqg8o3foshtdkmkmv4fczjgb2v8rfmcuvmnx\n",
      "Fetching next few tweets, next_token:  b26v89c19zqg8o3foshtdkm9mvhqvfzo6h15as1ol26il\n",
      "Fetching next few tweets, next_token:  b26v89c19zqg8o3foshtdkm9ibu3o03v2gjts7c7fozul\n",
      "Fetching next few tweets, next_token:  b26v89c19zqg8o3foshtdkm9ia4pm8iq3gpkjayh6p5z1\n",
      "Fetching next few tweets, next_token:  b26v89c19zqg8o3foshtdij1e5ctja8umofgw95huw5ml\n",
      "Fetching next few tweets, next_token:  b26v89c19zqg8o3foshtdihu8fn5tv91dvwnmlnthbl31\n",
      "Fetching next few tweets, next_token:  b26v89c19zqg8o3foshtdgeb7azgrrfnbejqj0fvuzvct\n",
      "Fetching next few tweets, next_token:  b26v89c19zqg8o3foshtdgcil9yntd4tmq18yvfwyhqbh\n",
      "Fetching next few tweets, next_token:  b26v89c19zqg8o3foshtde9l0fz0ac5i46oxzqf2775od\n",
      "Fetching next few tweets, next_token:  b26v89c19zqg8o3foshtde8e6v130ijqter526lqwapod\n",
      "Fetching next few tweets, next_token:  b26v89c19zqg8o3foshtde7shg78jtmpfdvifr31406m5\n",
      "Fetching next few tweets, next_token:  b26v89c19zqg8o3foshtde6wa9ujtfu2bs34t3czxvn25\n",
      "Fetching next few tweets, next_token:  b26v89c19zqg8o3foshtdc4uwm7ivhddw8vrnoms4it8d\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-cc4dc60c5508>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mnext_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_response\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'meta'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'next_token'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fetching next few tweets, next_token: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mquery_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'next_token'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mfile_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define headers for authorization\n",
    "headers = {\"Authorization\": \"Bearer \" + BEARER_TOKEN}\n",
    "\n",
    "print(\"Starting to fetch data\")\n",
    "\n",
    "\n",
    "while True:\n",
    "    # get results according to url and query\n",
    "    response = None\n",
    "    response = requests.request(\"GET\", url, headers=headers, params=query_params)\n",
    "              \n",
    "    if response.status_code != 200:\n",
    "         raise Exception(response.status_code, response.text)\n",
    "    \n",
    "    # create json out of result\n",
    "    json_response = response.json()\n",
    "    \n",
    "    # write data into txt\n",
    "    with open(path + \"twitter_file_\" + str(file_counter) + \".txt\",\n",
    "              \"w\") as outfile:\n",
    "        outfile.write(json.dumps(json_response, indent=4))\n",
    "\n",
    "    # check if more data available, if yes continue process\n",
    "    file_counter += 1\n",
    "    if 'meta' in json_response:\n",
    "        if 'next_token' in json_response['meta']:\n",
    "            query_params['next_token'] = json_response['meta']['next_token']\n",
    "            next_token = json_response['meta']['next_token']\n",
    "            print(\"Fetching next few tweets, next_token: \",query_params['next_token'])\n",
    "            time.sleep(4)\n",
    "        else:\n",
    "            file_counter = 0\n",
    "            break\n",
    "    else:\n",
    "        file_counter = 0\n",
    "        break\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-trace",
   "metadata": {},
   "source": [
    "## Creating the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "apart-trade",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['created_at'] = pd.to_datetime(df['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "breeding-ceramic",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>withheld.copyright</th>\n",
       "      <th>withheld.country_codes</th>\n",
       "      <th>withheld.scope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-28 23:38:16+00:00</td>\n",
       "      <td>1366170888387846144</td>\n",
       "      <td>#europe #switzerland #swiss #lugano #travel #f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-28 23:30:24+00:00</td>\n",
       "      <td>1366168909682991107</td>\n",
       "      <td>@petersankoff @lawandchocolate I have always w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-02-28 23:27:40+00:00</td>\n",
       "      <td>1366168219367714818</td>\n",
       "      <td>@IdeRetz @PoliticsForAlI @MetroUK Yes, it is J...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-28 23:13:59+00:00</td>\n",
       "      <td>1366164776670355456</td>\n",
       "      <td>UK has the Brazilian Covid19 variant in its mi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-28 23:11:47+00:00</td>\n",
       "      <td>1366164223353643015</td>\n",
       "      <td>RT @Snishaa2: ‡º∫‚úø‚òÜ‚úø\\rüíûüÖ∑üÖ∞üÖøüÖøüÜà üÜÉüÖ∑üÜÑüÜÅüÜÇüÖ≥üÖ∞üÜàüíû\\r‚úø\\r‚òÜ‚úø‡ºª\\n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>2008-02-10 19:04:47+00:00</td>\n",
       "      <td>696885762</td>\n",
       "      <td>@warzabidul Sounds like an exciting week. I lo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>2007-10-02 08:01:33+00:00</td>\n",
       "      <td>306691052</td>\n",
       "      <td>MaltaMedia.com: New travel regulations to Swit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>2007-06-27 15:22:29+00:00</td>\n",
       "      <td>123179132</td>\n",
       "      <td>Booking travel.  Denver, New York, Switzerland...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>2007-04-11 08:38:16+00:00</td>\n",
       "      <td>24449061</td>\n",
       "      <td>Vraiment f√¢ch√©e que Switzerland Travel Center ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>2007-03-08 13:52:35+00:00</td>\n",
       "      <td>5930768</td>\n",
       "      <td>booking more travel. Switzerland bound soon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>702694 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   created_at                   id  \\\n",
       "0   2021-02-28 23:38:16+00:00  1366170888387846144   \n",
       "1   2021-02-28 23:30:24+00:00  1366168909682991107   \n",
       "2   2021-02-28 23:27:40+00:00  1366168219367714818   \n",
       "3   2021-02-28 23:13:59+00:00  1366164776670355456   \n",
       "4   2021-02-28 23:11:47+00:00  1366164223353643015   \n",
       "..                        ...                  ...   \n",
       "441 2008-02-10 19:04:47+00:00            696885762   \n",
       "442 2007-10-02 08:01:33+00:00            306691052   \n",
       "443 2007-06-27 15:22:29+00:00            123179132   \n",
       "444 2007-04-11 08:38:16+00:00             24449061   \n",
       "445 2007-03-08 13:52:35+00:00              5930768   \n",
       "\n",
       "                                                  text withheld.copyright  \\\n",
       "0    #europe #switzerland #swiss #lugano #travel #f...                NaN   \n",
       "1    @petersankoff @lawandchocolate I have always w...                NaN   \n",
       "2    @IdeRetz @PoliticsForAlI @MetroUK Yes, it is J...                NaN   \n",
       "3    UK has the Brazilian Covid19 variant in its mi...                NaN   \n",
       "4    RT @Snishaa2: ‡º∫‚úø‚òÜ‚úø\\rüíûüÖ∑üÖ∞üÖøüÖøüÜà üÜÉüÖ∑üÜÑüÜÅüÜÇüÖ≥üÖ∞üÜàüíû\\r‚úø\\r‚òÜ‚úø‡ºª\\n...                NaN   \n",
       "..                                                 ...                ...   \n",
       "441  @warzabidul Sounds like an exciting week. I lo...                NaN   \n",
       "442  MaltaMedia.com: New travel regulations to Swit...                NaN   \n",
       "443  Booking travel.  Denver, New York, Switzerland...                NaN   \n",
       "444  Vraiment f√¢ch√©e que Switzerland Travel Center ...                NaN   \n",
       "445        booking more travel. Switzerland bound soon                NaN   \n",
       "\n",
       "    withheld.country_codes withheld.scope  \n",
       "0                      NaN            NaN  \n",
       "1                      NaN            NaN  \n",
       "2                      NaN            NaN  \n",
       "3                      NaN            NaN  \n",
       "4                      NaN            NaN  \n",
       "..                     ...            ...  \n",
       "441                    NaN            NaN  \n",
       "442                    NaN            NaN  \n",
       "443                    NaN            NaN  \n",
       "444                    NaN            NaN  \n",
       "445                    NaN            NaN  \n",
       "\n",
       "[702694 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "manufactured-cruise",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = pd.to_datetime(df.created_at).dt.year\n",
    "df['month'] = pd.to_datetime(df.created_at).dt.month\n",
    "\n",
    "data = df.groupby([\"year\", \"month\"]).size().to_frame(name='count').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "global-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"twitter_data.csv\", encoding=\"UTF-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "assumed-leave",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "[\n",
      "    {\n",
      "        \"author_id\": \"884330274850471936\",\n",
      "        \"created_at\": \"2021-10-13T23:59:58.000Z\",\n",
      "        \"id\": \"1448438395118800897\",\n",
      "        \"text\": \"RT @overwater001: New COVID cases in the last 24 hours\\nUK 42,776\\nGermany 5,818\\nNetherlands 3,716\\nItaly 2,772\\nPoland 2,640\\nAustria 2,614\\nLit\\u2026\"\n",
      "    },\n",
      "    {\n",
      "        \"author_id\": \"21986927\",\n",
      "        \"created_at\": \"2021-10-13T23:59:50.000Z\",\n",
      "        \"id\": \"1448438359232294918\",\n",
      "        \"text\": \"RT @earthcurated: A summer evening in Lauterbrunnen, Switzerland \\ud83c\\udde8\\ud83c\\udded https://t.co/hGLqmGSKQj\"\n",
      "    },\n",
      "    {\n",
      "        \"author_id\": \"339879512\",\n",
      "        \"created_at\": \"2021-10-13T23:59:14.000Z\",\n",
      "        \"id\": \"1448438209063661570\",\n",
      "        \"text\": \"@sportsdrenched @DigiEntertain There are levels.\\n\\nThe company I work for makes Automation machines all over the world.  \\n\\nIt\\u2019s almost cliche, but the basic entry level machines are made in China.\\n\\nMid level in Italy &amp; Argentina.\\n\\nTop level sophisticated machines are made in Germany, Switzerland &amp; the US.\"\n",
      "    },\n",
      "    {\n",
      "        \"author_id\": \"700459788916883457\",\n",
      "        \"created_at\": \"2021-10-13T23:58:51.000Z\",\n",
      "        \"id\": \"1448438111785074688\",\n",
      "        \"text\": \"Have you put Geneva, Switzerland on your bucket list yet? If not, get to it! :) https://t.co/cNQZe2EhJd #switzerland #travel https://t.co/q4UfL8ySuA\"\n",
      "    },\n",
      "    {\n",
      "        \"author_id\": \"281016719\",\n",
      "        \"created_at\": \"2021-10-13T23:58:35.000Z\",\n",
      "        \"id\": \"1448438044193918979\",\n",
      "        \"text\": \"RT @CSDefence_Kenya: 1/ Had occasion to receive H.E. Valentin Zellweger, Ambassador of Switzerland to Kenya in my office at Ulinzi House, t\\u2026\"\n",
      "    },\n",
      "    {\n",
      "        \"author_id\": \"1411727826559045639\",\n",
      "        \"created_at\": \"2021-10-13T23:58:20.000Z\",\n",
      "        \"id\": \"1448437982223077377\",\n",
      "        \"text\": \"@WazirXCares can you list DSFR, it is Switzerland based crypto coin has lot of potential. I want to buy. Even at Binance I don't mind. https://t.co/4WGwoEaYV6\"\n",
      "    },\n",
      "    {\n",
      "        \"author_id\": \"3013894540\",\n",
      "        \"created_at\": \"2021-10-13T23:58:18.000Z\",\n",
      "        \"id\": \"1448437975453511684\",\n",
      "        \"text\": \"@indy_sooner Lol, after 17 years mines like Switzerland. Game day she\\u2019s a rider, but laughs for fighting on here.\"\n",
      "    },\n",
      "    {\n",
      "        \"author_id\": \"1445360602243911689\",\n",
      "        \"created_at\": \"2021-10-13T23:57:02.000Z\",\n",
      "        \"id\": \"1448437656619204612\",\n",
      "        \"text\": \"RT @owzh_ch: Join my\\n\\nFree #German Learning Group A1-C2 on #Telegram \\n\\nwith Main Focus on Chatting and Teacher based in #Switzerland: https\\u2026\"\n",
      "    },\n",
      "    {\n",
      "        \"author_id\": \"166156539\",\n",
      "        \"created_at\": \"2021-10-13T23:56:56.000Z\",\n",
      "        \"id\": \"1448437632321658883\",\n",
      "        \"text\": \"RT @Chayenn72766251: Why is    C \\ud83d\\udd25E\\ud83d\\udd25R\\ud83d\\udd25N    such a big deal and secret?\\n\\nIt is the largest and highest Energy Particle Collider/Machine in t\\u2026\"\n",
      "    },\n",
      "    {\n",
      "        \"author_id\": \"839438123562061824\",\n",
      "        \"created_at\": \"2021-10-13T23:56:45.000Z\",\n",
      "        \"id\": \"1448437586423320578\",\n",
      "        \"text\": \"@car_los0427 @JackNathanMoran @Drewthanasia @GaryLineker Greece is like tier 4 in Europe and im telling u we could beat Ecuador Bolivia Venezuela Peru Paraguay. Imagine what a tier 2 teams like Switzerland or Croatia there, they would've guaranteed WC qualification\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# To set your environment variables in your terminal run the following line:\n",
    "# export 'BEARER_TOKEN'='<your_bearer_token>'\n",
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAAA2zUgEAAAAA8ZT2e%2By4ClxhUnhSn8Kww45qNDo%3DpdVd5VQ7aJElcpvMHABSyldfEJ4EPmUcPENMWxDgDaXIYAnbw2\"\n",
    "\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "\n",
    "# Optional params: start_time,end_time,since_id,until_id,max_results,next_token,\n",
    "# expansions,tweet.fields,media.fields,poll.fields,place.fields,user.fields\n",
    "\n",
    "query = 'switzerland'\n",
    "tweet_fields = 'created_at,author_id'\n",
    "start_time = '2021-10-13T00:00:00.000Z'\n",
    "end_time = '2021-10-13T23:59:59.000Z'\n",
    "max_results = '10'\n",
    "\n",
    "\n",
    "query_params = {'query': query,'tweet.fields': tweet_fields,\\\n",
    "                'start_time': start_time, 'end_time': end_time, 'max_results': max_results}\n",
    "\n",
    "\n",
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2FullArchiveSearchPython\"\n",
    "    return r\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url, params):\n",
    "    response = requests.request(\"GET\", search_url, auth=bearer_oauth, params=params)\n",
    "    print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def main():\n",
    "    json_response = connect_to_endpoint(search_url, query_params)\n",
    "    print(json.dumps(json_response['data'], indent=4, sort_keys=True))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-scroll",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
