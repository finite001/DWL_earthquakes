{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1270f0f8-9022-46ae-ae35-26624b2df0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import dotenv_values\n",
    "from sqlalchemy import create_engine\n",
    "import requests\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c226ad16-aabb-4b0e-9afc-95d7371cc426",
   "metadata": {},
   "source": [
    "### Connection to AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41a0cb0f-5ad5-4bcf-8acf-87ad5ab842e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the connection details for the rds db from .env file\n",
    "config = dotenv_values(\".env\")  \n",
    "HOST_RDS = config['HOST_RDS']\n",
    "DBNAME_RDS = config['DBNAME_RDS']\n",
    "USER_RDS = config['USER_RDS']\n",
    "PASSWORD_RDS = config['PASSWORD_RDS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48be2f06-0c64-4a3f-a4df-8aa5e742a743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df from postgreSQL tables\n",
    "engine_str = 'postgresql+psycopg2://' + USER_RDS + ':' + PASSWORD_RDS + \"@\" + HOST_RDS + \":5432/dbeq\"\n",
    "engine = create_engine(engine_str)\n",
    "dbConnection = engine.connect()\n",
    "twitter = pd.read_sql(\"SELECT * FROM tweets11\", dbConnection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bb0dc88-0f84-4d37-8c47-e1da83d89ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      text  author_id  \\\n",
      "2458888                                http://t.co/VwOeojT  237523276   \n",
      "2469741                                 Wat earthquake????  259471703   \n",
      "2472683                                      Earthquake!!!   80360865   \n",
      "2472684                                        earthquake!  312806750   \n",
      "2472685                                        Earthquake?   98770628   \n",
      "...                                                    ...        ...   \n",
      "3276922  RT @BreakingNews: Earthquake, magnitude 6.6, r...   48024534   \n",
      "3276923  Oh wow. I srsly thought the earthquake(s) @ 4/...   32933200   \n",
      "3276924  #earthquake #NZ: Magnitude 3.0, Tuesday, July ...  265168756   \n",
      "3276925  #earthquake #NZ: Magnitude 3.3, Tuesday, July ...  265168756   \n",
      "3276926  UPDATE 1 — Strong earthquake hits off the Phil...  248387799   \n",
      "\n",
      "                        id                created_at  \n",
      "2458888  95850883149082624  2011-07-26T13:40:06.000Z  \n",
      "2469741  94674263612727296  2011-07-23T07:44:38.000Z  \n",
      "2472683  94672362196303872  2011-07-23T07:37:04.000Z  \n",
      "2472684  94672362007572480  2011-07-23T07:37:04.000Z  \n",
      "2472685  94672361265180672  2011-07-23T07:37:04.000Z  \n",
      "...                    ...                       ...  \n",
      "3276922  90593245679198208  2011-07-12T01:28:07.000Z  \n",
      "3276923  90593192092766208  2011-07-12T01:27:54.000Z  \n",
      "3276924  90593159498842112  2011-07-12T01:27:47.000Z  \n",
      "3276925  90593157129056257  2011-07-12T01:27:46.000Z  \n",
      "3276926  90593118893785088  2011-07-12T01:27:37.000Z  \n",
      "\n",
      "[744000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(twitter[twitter.duplicated(subset=['id'])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f068ba6-3733-437a-86a0-30b39d1c93d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [text, author_id, id, created_at]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# removing duplicates (users where extracted multiple times because of 500 tweets per request limit)\n",
    "twitter.drop_duplicates(subset=['id'], inplace=True)\n",
    "print(twitter[twitter.duplicated(subset=['id'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77b7905d-3149-43bc-b68e-62f5970d6c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine tables\n",
    "df = pd.merge(twitter, users, how='left', left_on='author_id', right_on='id')\n",
    "df = df.rename(columns={\"id_x\": \"tweet_id\"}).drop(columns=['id_y'])\n",
    "# drop column id from users side, as it is the same as author_id from twitter table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8805c058-6779-4033-863e-ec44d4d0b657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text          object\n",
       "author_id      int64\n",
       "tweet_id       int64\n",
       "created_at    object\n",
       "username      object\n",
       "name          object\n",
       "location      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4da0b005-fb8d-4f3d-ad68-3ce2542ba71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change created_at dtype\n",
    "df['created_at'] = pd.to_datetime(df['created_at']).dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "abb5e7ba-b6a6-42bb-8dcc-05347c1c2fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>username</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ｗｈｅｎ ｉ ｆｅｅｌ ｅａｒｔｈｑｕａｋｅ， ｍｙ ｈａｎｄ ａｕｔｏｍａｔｉｃａｌｌｙ ...</td>\n",
       "      <td>2760893045</td>\n",
       "      <td>884990764933148673</td>\n",
       "      <td>2017-07-12 04:20:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ｗｈｅｎ ｉ ｆｅｅｌ ｅａｒｔｈｑｕａｋｅ， ｍｙ ｈａｎｄ ａｕｔｏｍａｔｉｃａｌｌｙ ...</td>\n",
       "      <td>3214732451</td>\n",
       "      <td>921250488313622528</td>\n",
       "      <td>2017-10-20 05:43:27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ｗｈｅｎ　ｉ　ｆｅｅｌ　ｅａｒｔｈｑｕａｋｅ，　ｍｙ　ｈａｎｄ　ａｕｔｏｍａｔｉｃａｌｌｙ　...</td>\n",
       "      <td>36488329</td>\n",
       "      <td>928040346604523520</td>\n",
       "      <td>2017-11-07 23:23:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ｗｈｅｎ　ｉ　ｆｅｅｌ　ｅａｒｔｈｑｕａｋｅ，　ｍｙ　ｈａｎｄ　ａｕｔｏｍａｔｉｃａｌｌｙ　...</td>\n",
       "      <td>308947696</td>\n",
       "      <td>928030577537589248</td>\n",
       "      <td>2017-11-07 22:45:06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>ｗｈｅｎ　ｉ　ｆｅｅｌ　ｅａｒｔｈｑｕａｋｅ，　ｍｙ　ｈａｎｄ　ａｕｔｏｍａｔｉｃａｌｌｙ　...</td>\n",
       "      <td>3062164543</td>\n",
       "      <td>938577830752935936</td>\n",
       "      <td>2017-12-07 01:16:07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13517310</th>\n",
       "      <td>鹿児島で震度5強。：https://t.co/EKgCD3bD0Q鹿児島で震度5強。またです...</td>\n",
       "      <td>266278707</td>\n",
       "      <td>885140873863872512</td>\n",
       "      <td>2017-07-12 14:16:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13517311</th>\n",
       "      <td>鹿児島　震度5強\\n\\nhttps://t.co/vm5PTMfJuh https://t....</td>\n",
       "      <td>875992792773738496</td>\n",
       "      <td>884609069700403200</td>\n",
       "      <td>2017-07-11 03:03:32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13517313</th>\n",
       "      <td>鹿児島で震度5強\\n\\n九州は散々やで…(*_*)\\n\\n地震情報 - Yahoo!天気・災...</td>\n",
       "      <td>86334263</td>\n",
       "      <td>884609347925426177</td>\n",
       "      <td>2017-07-11 03:04:38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13517315</th>\n",
       "      <td>鹿児島で震度5強の地震\\nhttps://t.co/Gl9wT2IQz4</td>\n",
       "      <td>2414138750</td>\n",
       "      <td>884609357027041281</td>\n",
       "      <td>2017-07-11 03:04:40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13517317</th>\n",
       "      <td>鹿児島で震度5強。\\n津波の心配なし。\\nhttps://t.co/kVOocEtyma</td>\n",
       "      <td>150649490</td>\n",
       "      <td>884608421156147201</td>\n",
       "      <td>2017-07-11 03:00:57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>309020 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       text  \\\n",
       "4         ｗｈｅｎ ｉ ｆｅｅｌ ｅａｒｔｈｑｕａｋｅ， ｍｙ ｈａｎｄ ａｕｔｏｍａｔｉｃａｌｌｙ ...   \n",
       "5         ｗｈｅｎ ｉ ｆｅｅｌ ｅａｒｔｈｑｕａｋｅ， ｍｙ ｈａｎｄ ａｕｔｏｍａｔｉｃａｌｌｙ ...   \n",
       "23        ｗｈｅｎ　ｉ　ｆｅｅｌ　ｅａｒｔｈｑｕａｋｅ，　ｍｙ　ｈａｎｄ　ａｕｔｏｍａｔｉｃａｌｌｙ　...   \n",
       "30        ｗｈｅｎ　ｉ　ｆｅｅｌ　ｅａｒｔｈｑｕａｋｅ，　ｍｙ　ｈａｎｄ　ａｕｔｏｍａｔｉｃａｌｌｙ　...   \n",
       "49        ｗｈｅｎ　ｉ　ｆｅｅｌ　ｅａｒｔｈｑｕａｋｅ，　ｍｙ　ｈａｎｄ　ａｕｔｏｍａｔｉｃａｌｌｙ　...   \n",
       "...                                                     ...   \n",
       "13517310  鹿児島で震度5強。：https://t.co/EKgCD3bD0Q鹿児島で震度5強。またです...   \n",
       "13517311  鹿児島　震度5強\\n\\nhttps://t.co/vm5PTMfJuh https://t....   \n",
       "13517313  鹿児島で震度5強\\n\\n九州は散々やで…(*_*)\\n\\n地震情報 - Yahoo!天気・災...   \n",
       "13517315               鹿児島で震度5強の地震\\nhttps://t.co/Gl9wT2IQz4   \n",
       "13517317       鹿児島で震度5強。\\n津波の心配なし。\\nhttps://t.co/kVOocEtyma   \n",
       "\n",
       "                   author_id            tweet_id          created_at username  \\\n",
       "4                 2760893045  884990764933148673 2017-07-12 04:20:15      NaN   \n",
       "5                 3214732451  921250488313622528 2017-10-20 05:43:27      NaN   \n",
       "23                  36488329  928040346604523520 2017-11-07 23:23:55      NaN   \n",
       "30                 308947696  928030577537589248 2017-11-07 22:45:06      NaN   \n",
       "49                3062164543  938577830752935936 2017-12-07 01:16:07      NaN   \n",
       "...                      ...                 ...                 ...      ...   \n",
       "13517310           266278707  885140873863872512 2017-07-12 14:16:44      NaN   \n",
       "13517311  875992792773738496  884609069700403200 2017-07-11 03:03:32      NaN   \n",
       "13517313            86334263  884609347925426177 2017-07-11 03:04:38      NaN   \n",
       "13517315          2414138750  884609357027041281 2017-07-11 03:04:40      NaN   \n",
       "13517317           150649490  884608421156147201 2017-07-11 03:00:57      NaN   \n",
       "\n",
       "         name  \n",
       "4         NaN  \n",
       "5         NaN  \n",
       "23        NaN  \n",
       "30        NaN  \n",
       "49        NaN  \n",
       "...       ...  \n",
       "13517310  NaN  \n",
       "13517311  NaN  \n",
       "13517313  NaN  \n",
       "13517315  NaN  \n",
       "13517317  NaN  \n",
       "\n",
       "[309020 rows x 6 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some users could not be inserted into postgreSQL table due to invalid characters. We identify them:\n",
    "df[df['username'].isna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fbfd49b-55b4-4086-9a44-9ba7173f2bed",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.date(2017, 7, 1) datetime.date(2017, 7, 2)\n",
      " datetime.date(2017, 7, 3) datetime.date(2017, 7, 4)\n",
      " datetime.date(2017, 7, 5) datetime.date(2017, 7, 6)\n",
      " datetime.date(2017, 7, 7) datetime.date(2017, 7, 8)\n",
      " datetime.date(2017, 7, 9) datetime.date(2017, 7, 10)\n",
      " datetime.date(2017, 7, 11) datetime.date(2017, 7, 12)\n",
      " datetime.date(2017, 7, 13) datetime.date(2017, 7, 14)\n",
      " datetime.date(2017, 7, 15) datetime.date(2017, 7, 16)\n",
      " datetime.date(2017, 7, 17) datetime.date(2017, 7, 18)\n",
      " datetime.date(2017, 7, 19) datetime.date(2017, 7, 20)\n",
      " datetime.date(2017, 7, 21) datetime.date(2017, 7, 22)\n",
      " datetime.date(2017, 7, 23) datetime.date(2017, 7, 24)\n",
      " datetime.date(2017, 7, 25) datetime.date(2017, 7, 26)\n",
      " datetime.date(2017, 7, 27) datetime.date(2017, 7, 28)\n",
      " datetime.date(2017, 7, 29) datetime.date(2017, 7, 30)\n",
      " datetime.date(2017, 7, 31) datetime.date(2017, 8, 1)\n",
      " datetime.date(2017, 8, 2) datetime.date(2017, 8, 3)\n",
      " datetime.date(2017, 8, 4) datetime.date(2017, 8, 5)\n",
      " datetime.date(2017, 8, 6) datetime.date(2017, 8, 7)\n",
      " datetime.date(2017, 8, 8) datetime.date(2017, 8, 9)\n",
      " datetime.date(2017, 8, 10) datetime.date(2017, 8, 11)\n",
      " datetime.date(2017, 8, 12) datetime.date(2017, 8, 13)\n",
      " datetime.date(2017, 8, 14) datetime.date(2017, 8, 15)\n",
      " datetime.date(2017, 8, 16) datetime.date(2017, 8, 17)\n",
      " datetime.date(2017, 8, 18) datetime.date(2017, 8, 19)\n",
      " datetime.date(2017, 8, 20) datetime.date(2017, 8, 21)\n",
      " datetime.date(2017, 8, 22) datetime.date(2017, 8, 23)\n",
      " datetime.date(2017, 8, 24) datetime.date(2017, 8, 25)\n",
      " datetime.date(2017, 8, 26) datetime.date(2017, 8, 27)\n",
      " datetime.date(2017, 8, 28) datetime.date(2017, 8, 29)\n",
      " datetime.date(2017, 8, 30) datetime.date(2017, 8, 31)\n",
      " datetime.date(2017, 9, 1) datetime.date(2017, 9, 2)\n",
      " datetime.date(2017, 9, 3) datetime.date(2017, 9, 4)\n",
      " datetime.date(2017, 9, 5) datetime.date(2017, 9, 6)\n",
      " datetime.date(2017, 9, 7) datetime.date(2017, 9, 8)\n",
      " datetime.date(2017, 9, 9) datetime.date(2017, 9, 10)\n",
      " datetime.date(2017, 9, 11) datetime.date(2017, 9, 12)\n",
      " datetime.date(2017, 9, 13) datetime.date(2017, 9, 14)\n",
      " datetime.date(2017, 9, 15) datetime.date(2017, 9, 16)\n",
      " datetime.date(2017, 9, 17) datetime.date(2017, 9, 18)\n",
      " datetime.date(2017, 9, 19) datetime.date(2017, 9, 20)\n",
      " datetime.date(2017, 9, 21) datetime.date(2017, 9, 22)\n",
      " datetime.date(2017, 9, 23) datetime.date(2017, 9, 24)\n",
      " datetime.date(2017, 9, 25) datetime.date(2017, 9, 26)\n",
      " datetime.date(2017, 9, 27) datetime.date(2017, 9, 28)\n",
      " datetime.date(2017, 9, 29) datetime.date(2017, 9, 30)\n",
      " datetime.date(2017, 10, 1) datetime.date(2017, 10, 2)\n",
      " datetime.date(2017, 10, 3) datetime.date(2017, 10, 4)\n",
      " datetime.date(2017, 10, 5) datetime.date(2017, 10, 6)\n",
      " datetime.date(2017, 10, 7) datetime.date(2017, 10, 8)\n",
      " datetime.date(2017, 10, 9) datetime.date(2017, 10, 10)\n",
      " datetime.date(2017, 10, 11) datetime.date(2017, 10, 12)\n",
      " datetime.date(2017, 10, 13) datetime.date(2017, 10, 14)\n",
      " datetime.date(2017, 10, 15) datetime.date(2017, 10, 16)\n",
      " datetime.date(2017, 10, 17) datetime.date(2017, 10, 18)\n",
      " datetime.date(2017, 10, 19) datetime.date(2017, 10, 20)\n",
      " datetime.date(2017, 10, 21) datetime.date(2017, 10, 22)\n",
      " datetime.date(2017, 10, 23) datetime.date(2017, 10, 24)\n",
      " datetime.date(2017, 10, 25) datetime.date(2017, 10, 26)\n",
      " datetime.date(2017, 10, 27) datetime.date(2017, 10, 28)\n",
      " datetime.date(2017, 10, 29) datetime.date(2017, 10, 30)\n",
      " datetime.date(2017, 10, 31) datetime.date(2017, 11, 1)\n",
      " datetime.date(2017, 11, 2) datetime.date(2017, 11, 3)\n",
      " datetime.date(2017, 11, 4) datetime.date(2017, 11, 5)\n",
      " datetime.date(2017, 11, 6) datetime.date(2017, 11, 7)\n",
      " datetime.date(2017, 11, 8) datetime.date(2017, 11, 9)\n",
      " datetime.date(2017, 11, 10) datetime.date(2017, 11, 11)\n",
      " datetime.date(2017, 11, 12) datetime.date(2017, 11, 13)\n",
      " datetime.date(2017, 11, 14) datetime.date(2017, 11, 15)\n",
      " datetime.date(2017, 11, 16) datetime.date(2017, 11, 17)\n",
      " datetime.date(2017, 11, 18) datetime.date(2017, 11, 19)\n",
      " datetime.date(2017, 11, 20) datetime.date(2017, 11, 21)\n",
      " datetime.date(2017, 11, 22) datetime.date(2017, 11, 23)\n",
      " datetime.date(2017, 11, 24) datetime.date(2017, 11, 25)\n",
      " datetime.date(2017, 11, 26) datetime.date(2017, 11, 27)\n",
      " datetime.date(2017, 11, 28) datetime.date(2017, 11, 29)\n",
      " datetime.date(2017, 11, 30) datetime.date(2017, 12, 1)\n",
      " datetime.date(2017, 12, 2) datetime.date(2017, 12, 3)\n",
      " datetime.date(2017, 12, 4) datetime.date(2017, 12, 5)\n",
      " datetime.date(2017, 12, 6) datetime.date(2017, 12, 7)\n",
      " datetime.date(2017, 12, 8) datetime.date(2017, 12, 9)\n",
      " datetime.date(2017, 12, 10) datetime.date(2017, 12, 11)\n",
      " datetime.date(2017, 12, 12) datetime.date(2017, 12, 13)\n",
      " datetime.date(2017, 12, 14) datetime.date(2017, 12, 15)\n",
      " datetime.date(2017, 12, 16) datetime.date(2017, 12, 17)\n",
      " datetime.date(2017, 12, 18) datetime.date(2017, 12, 19)\n",
      " datetime.date(2017, 12, 20) datetime.date(2017, 12, 21)\n",
      " datetime.date(2017, 12, 22) datetime.date(2017, 12, 23)\n",
      " datetime.date(2017, 12, 24) datetime.date(2017, 12, 25)\n",
      " datetime.date(2017, 12, 26) datetime.date(2017, 12, 27)\n",
      " datetime.date(2017, 12, 28) datetime.date(2017, 12, 29)\n",
      " datetime.date(2017, 12, 30) datetime.date(2017, 12, 31)\n",
      " datetime.date(2018, 10, 1) datetime.date(2018, 10, 2)\n",
      " datetime.date(2018, 10, 3) datetime.date(2018, 10, 4)\n",
      " datetime.date(2018, 10, 5) datetime.date(2018, 10, 6)\n",
      " datetime.date(2018, 10, 7) datetime.date(2018, 10, 8)\n",
      " datetime.date(2018, 10, 9) datetime.date(2018, 10, 10)\n",
      " datetime.date(2018, 10, 11) datetime.date(2018, 10, 12)\n",
      " datetime.date(2018, 10, 13) datetime.date(2018, 10, 14)\n",
      " datetime.date(2018, 10, 15) datetime.date(2018, 10, 16)\n",
      " datetime.date(2018, 10, 17) datetime.date(2018, 10, 18)\n",
      " datetime.date(2018, 10, 19) datetime.date(2018, 10, 20)\n",
      " datetime.date(2018, 10, 21) datetime.date(2018, 10, 22)\n",
      " datetime.date(2018, 10, 23) datetime.date(2018, 10, 24)\n",
      " datetime.date(2018, 10, 25) datetime.date(2018, 10, 26)\n",
      " datetime.date(2018, 10, 27) datetime.date(2018, 10, 28)\n",
      " datetime.date(2018, 10, 29) datetime.date(2018, 10, 30)\n",
      " datetime.date(2018, 10, 31) datetime.date(2018, 11, 1)\n",
      " datetime.date(2018, 11, 2) datetime.date(2018, 11, 3)\n",
      " datetime.date(2018, 11, 4) datetime.date(2018, 11, 5)\n",
      " datetime.date(2018, 11, 6) datetime.date(2018, 11, 7)\n",
      " datetime.date(2018, 11, 8) datetime.date(2018, 11, 9)\n",
      " datetime.date(2018, 11, 10) datetime.date(2018, 11, 11)\n",
      " datetime.date(2018, 11, 12) datetime.date(2018, 11, 13)\n",
      " datetime.date(2021, 7, 9)]\n"
     ]
    }
   ],
   "source": [
    "# we could either delete those tweets or try to insert those users. Since we have access to the data, we will insert it. However, getting the users by author_id would take too long, as there are \n",
    "# 240239 unique users missing. We will rather get the tweets with the users again for that timeframe, remove the invalid character and insert it into the table.\n",
    "missing = df[df.isna().any(axis=1)].copy()\n",
    "missing['created_at'] = pd.to_datetime(missing['created_at']).dt.date\n",
    "missing.sort_values(by='created_at', inplace=True)\n",
    "\n",
    "uniqueDates = missing['created_at'].unique()\n",
    "print(uniqueDates)\n",
    "# from the output we see the affected time periods are: 1.7.17-31.12.17, 1.7.18-13.10.18 and then one day on 9.7.21 (these we will delete, they are only a few tweets and won't change the analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cb4e8f-47ea-4455-990b-c862263024b7",
   "metadata": {},
   "source": [
    "### Extract additional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ab9b2de-4a80-40d0-8915-3ccc51f05d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEARER_TOKEN = config['BEARER_TOKEN']\n",
    "FILTER_QUERY = \"-from:quakeupdates -from:jojo2727 -from:MonitorSismico -from:MyComicalLife -from:news_sokuho_bot -from:DiariosRobot -from:EN_NERV -from:GDACS -from:earthquake_jp -from:EQAlerts -from:j1_quake -from:iSachinSrivstva -from:VolcanoEWS -from:ChileAlertaApp -from:earthb0t -from:sexy_vegetables -from:zishin3255 -from:everyEarthquake -from:MapQuake -from:swap_bot_bash -from:eq_map -from:eq_map_es -from:eq_map_ww -from:SEISMOinfo -from:VegaBajaWx -from:WatchOurCity -from:Keith_Event -from:SismoDetector -from:cvb_223 -from:ExBulletinUk -from:EMSC -from:StoixeioJewelry -from:megamodo -from:earthquakevt -from:QuakeBotter -from:twtaka_jp -from:EarthquakeTw -from:ENSO1998 -from:eq_map_ww2 -from:eq_map_es2\"\n",
    "\n",
    "\n",
    "start_time = '2017-07-01T00:00:00.000Z'\n",
    "end_time = \"2017-12-31T23:59:59.000Z\"\n",
    "query = \"earthquake -minor, -is:reply -is:retweet {0}\".format(FILTER_QUERY)\n",
    "max_results = \"500\"\n",
    "tweet_fields = \"created_at,author_id\"\n",
    "user_fields = 'username,location'\n",
    "expansions = 'author_id'\n",
    "counter = 0\n",
    "query_params = {'query': query, 'tweet.fields': tweet_fields, 'user.fields': user_fields, \\\n",
    "                'start_time': start_time, 'end_time': end_time, 'max_results': max_results, \\\n",
    "                'expansions': expansions}\n",
    "url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "headers = {\"Authorization\": \"Bearer \" + BEARER_TOKEN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0ea515e-c5c6-47f8-9012-f231bed0666c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lst_tweets = []\n",
    "lst_users = []\n",
    "while True:\n",
    "    # get results according to url and query\n",
    "    response = requests.request(\"GET\", url, headers=headers, params=query_params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "\n",
    "    # combine data to one\n",
    "    json_response = response.json()\n",
    "    if 'data' in json_response:\n",
    "        lst_tweets = lst_tweets + json_response['data']\n",
    "        lst_users = lst_users + json_response['includes']['users']\n",
    "\n",
    "    # check if more data available, if yes continue process\n",
    "    if 'meta' in json_response:\n",
    "        if 'next_token' in json_response['meta']:\n",
    "            query_params['next_token'] = json_response['meta']['next_token']\n",
    "            next_token = json_response['meta']['next_token']\n",
    "          #  logging.info(\"Fetching next few tweets, next_token: \", query_params['next_token'])\n",
    "            time.sleep(5)\n",
    "        else:\n",
    "            if 'next_token' in query_params:\n",
    "                del query_params['next_token']\n",
    "            break\n",
    "    else:\n",
    "        if 'next_token' in query_params:\n",
    "            del query_params['next_token']\n",
    "            print('no meta in json_response')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcce1d2-afc0-494a-a2a6-71c0bd07dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = pd.DataFrame(lst_tweets)\n",
    "df_u = pd.DataFrame(lst_users)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8c6d9b-f68a-4a06-b474-40dac5d511dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# because of memory issues the data was saved to csv and will be read in here again\n",
    "twe = pd.read_csv(\"matchingtweets.csv\")\n",
    "use = pd.read_csv(\"missingusers.csv\")\n",
    "\n",
    "df2 = pd.merge(twe, use, how='left', left_on='author_id', right_on='id')\n",
    "df2 = df2.rename(columns={\"id_x\": \"tweet_id\"}).drop(columns=['id_y'])\n",
    "\n",
    "# delete location because of no relevant information\n",
    "df2.drop(columns=['location'], inplace=True)\n",
    "df.drop(columns=['location'], inplace=True)\n",
    "\n",
    "df2['created_at'] = pd.to_datetime(df2['created_at']).dt.tz_localize(None)\n",
    "\n",
    "df3 = df2.append(df)\n",
    "df3.drop_duplicates(subset=['tweet_id'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a0c5df-b457-4015-bd8e-d6a22a54b5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>username</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>776569</th>\n",
       "      <td>911360842528100352</td>\n",
       "      <td>2017-09-22 22:45:31</td>\n",
       "      <td>The reason Frida, the trapped little girl, gri...</td>\n",
       "      <td>186552155</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jeff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26472</th>\n",
       "      <td>1059683238170542080</td>\n",
       "      <td>2018-11-06 05:45:46</td>\n",
       "      <td>02/14/14: I’m watching House of Cards (S2) in ...</td>\n",
       "      <td>156334774</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54200</th>\n",
       "      <td>1056912735244562434</td>\n",
       "      <td>2018-10-29 14:16:47</td>\n",
       "      <td>0.4, 0.8, 1.1 \\nIf this appears on #OnlyConnec...</td>\n",
       "      <td>310455504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64102</th>\n",
       "      <td>1054783429970137088</td>\n",
       "      <td>2018-10-23 17:15:41</td>\n",
       "      <td>0.4 level earthquake today @NoFrackLancs  Time...</td>\n",
       "      <td>4331730273</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106104</th>\n",
       "      <td>1052907078615031808</td>\n",
       "      <td>2018-10-18 12:59:44</td>\n",
       "      <td>074 // earthquake // #100daysofascent #100dayc...</td>\n",
       "      <td>23904102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13514569</th>\n",
       "      <td>1052127631221514241</td>\n",
       "      <td>2018-10-16 09:22:29</td>\n",
       "      <td>「ドーン！」と音がしただけで何かよくわからなかったけど、最近多いらしい…\\n大地震の前兆とか...</td>\n",
       "      <td>550738293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13514660</th>\n",
       "      <td>1061534708427706369</td>\n",
       "      <td>2018-11-11 08:22:51</td>\n",
       "      <td>この頃、また地震が多い…。\\n地震情報 - Yahoo!天気・災害 https://t.co...</td>\n",
       "      <td>110049763</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13515815</th>\n",
       "      <td>1058274420068741120</td>\n",
       "      <td>2018-11-02 08:27:38</td>\n",
       "      <td>カラスが騒がしかったのと、家鳴りがあったのはこれかな？\\n地震情報 - Yahoo!天気・災...</td>\n",
       "      <td>2966046025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13516249</th>\n",
       "      <td>1058266760590254080</td>\n",
       "      <td>2018-11-02 07:57:12</td>\n",
       "      <td>高知市内揺れたね、和歌山の方が震源みたい。#地震 #和歌山 #震度4 https://t.c...</td>\n",
       "      <td>198874833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13517159</th>\n",
       "      <td>884608773515395074</td>\n",
       "      <td>2017-07-11 03:02:21</td>\n",
       "      <td>鹿児島湾で地震発生（2017年7月11日 11時56分）最大震度5強 https://t.c...</td>\n",
       "      <td>237587677</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18653 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     tweet_id          created_at  \\\n",
       "776569     911360842528100352 2017-09-22 22:45:31   \n",
       "26472     1059683238170542080 2018-11-06 05:45:46   \n",
       "54200     1056912735244562434 2018-10-29 14:16:47   \n",
       "64102     1054783429970137088 2018-10-23 17:15:41   \n",
       "106104    1052907078615031808 2018-10-18 12:59:44   \n",
       "...                       ...                 ...   \n",
       "13514569  1052127631221514241 2018-10-16 09:22:29   \n",
       "13514660  1061534708427706369 2018-11-11 08:22:51   \n",
       "13515815  1058274420068741120 2018-11-02 08:27:38   \n",
       "13516249  1058266760590254080 2018-11-02 07:57:12   \n",
       "13517159   884608773515395074 2017-07-11 03:02:21   \n",
       "\n",
       "                                                       text   author_id  \\\n",
       "776569    The reason Frida, the trapped little girl, gri...   186552155   \n",
       "26472     02/14/14: I’m watching House of Cards (S2) in ...   156334774   \n",
       "54200     0.4, 0.8, 1.1 \\nIf this appears on #OnlyConnec...   310455504   \n",
       "64102     0.4 level earthquake today @NoFrackLancs  Time...  4331730273   \n",
       "106104    074 // earthquake // #100daysofascent #100dayc...    23904102   \n",
       "...                                                     ...         ...   \n",
       "13514569  「ドーン！」と音がしただけで何かよくわからなかったけど、最近多いらしい…\\n大地震の前兆とか...   550738293   \n",
       "13514660  この頃、また地震が多い…。\\n地震情報 - Yahoo!天気・災害 https://t.co...   110049763   \n",
       "13515815  カラスが騒がしかったのと、家鳴りがあったのはこれかな？\\n地震情報 - Yahoo!天気・災...  2966046025   \n",
       "13516249  高知市内揺れたね、和歌山の方が震源みたい。#地震 #和歌山 #震度4 https://t.c...   198874833   \n",
       "13517159  鹿児島湾で地震発生（2017年7月11日 11時56分）最大震度5強 https://t.c...   237587677   \n",
       "\n",
       "         username  name  \n",
       "776569        NaN  jeff  \n",
       "26472         NaN   NaN  \n",
       "54200         NaN   NaN  \n",
       "64102         NaN   NaN  \n",
       "106104        NaN   NaN  \n",
       "...           ...   ...  \n",
       "13514569      NaN   NaN  \n",
       "13514660      NaN   NaN  \n",
       "13515815      NaN   NaN  \n",
       "13516249      NaN   NaN  \n",
       "13517159      NaN   NaN  \n",
       "\n",
       "[18653 rows x 6 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for nan again, on column username\n",
    "df3[df3['username'].isna()] # significantly less. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26caf8d1-22be-492f-849c-b00882d54dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv('temp_df3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a12c408-19bd-4c66-bfd9-f7cd198631ea",
   "metadata": {},
   "source": [
    "## Remove Bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05e93c92-18d0-45fb-b798-c47b1fc7d67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#2011memories the earthquake when i was at @th...</td>\n",
       "      <td>152378466</td>\n",
       "      <td>153264156865339395</td>\n",
       "      <td>2011-12-31 23:59:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Please know there are many people still facing...</td>\n",
       "      <td>266919423</td>\n",
       "      <td>153263988745056256</td>\n",
       "      <td>2011-12-31 23:59:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I think the earthquake hurt my stuffed animals...</td>\n",
       "      <td>30585588</td>\n",
       "      <td>153263967727394816</td>\n",
       "      <td>2011-12-31 23:59:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#YourNewsTweet -  2nd quake in a week rattles ...</td>\n",
       "      <td>380125872</td>\n",
       "      <td>153263927428530176</td>\n",
       "      <td>2011-12-31 23:59:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>that earthquake woke me up to a new life</td>\n",
       "      <td>324587833</td>\n",
       "      <td>153263924681252864</td>\n",
       "      <td>2011-12-31 23:59:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5612799</th>\n",
       "      <td>NEW World earthquake forecasts updated for Jan...</td>\n",
       "      <td>37501849</td>\n",
       "      <td>20994889508458496</td>\n",
       "      <td>2011-01-01 00:09:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5612800</th>\n",
       "      <td>waiting for the earthquake like @jeiheel</td>\n",
       "      <td>56163369</td>\n",
       "      <td>20994734549901313</td>\n",
       "      <td>2011-01-01 00:08:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5612801</th>\n",
       "      <td>US scientists warned in 2008 that Port-au-Prin...</td>\n",
       "      <td>26184071</td>\n",
       "      <td>20994614701850625</td>\n",
       "      <td>2011-01-01 00:08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5612802</th>\n",
       "      <td>#2010Memories traveling to Haiti after earthqu...</td>\n",
       "      <td>20287993</td>\n",
       "      <td>20994577573871616</td>\n",
       "      <td>2011-01-01 00:07:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5612803</th>\n",
       "      <td>Recent Worldwide Earthquake List - USGS : M 5....</td>\n",
       "      <td>18916829</td>\n",
       "      <td>20993318066327552</td>\n",
       "      <td>2011-01-01 00:02:51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4868804 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  author_id  \\\n",
       "0        #2011memories the earthquake when i was at @th...  152378466   \n",
       "1        Please know there are many people still facing...  266919423   \n",
       "2        I think the earthquake hurt my stuffed animals...   30585588   \n",
       "3        #YourNewsTweet -  2nd quake in a week rattles ...  380125872   \n",
       "4                 that earthquake woke me up to a new life  324587833   \n",
       "...                                                    ...        ...   \n",
       "5612799  NEW World earthquake forecasts updated for Jan...   37501849   \n",
       "5612800           waiting for the earthquake like @jeiheel   56163369   \n",
       "5612801  US scientists warned in 2008 that Port-au-Prin...   26184071   \n",
       "5612802  #2010Memories traveling to Haiti after earthqu...   20287993   \n",
       "5612803  Recent Worldwide Earthquake List - USGS : M 5....   18916829   \n",
       "\n",
       "                         id          created_at  \n",
       "0        153264156865339395 2011-12-31 23:59:57  \n",
       "1        153263988745056256 2011-12-31 23:59:17  \n",
       "2        153263967727394816 2011-12-31 23:59:12  \n",
       "3        153263927428530176 2011-12-31 23:59:02  \n",
       "4        153263924681252864 2011-12-31 23:59:01  \n",
       "...                     ...                 ...  \n",
       "5612799   20994889508458496 2011-01-01 00:09:06  \n",
       "5612800   20994734549901313 2011-01-01 00:08:29  \n",
       "5612801   20994614701850625 2011-01-01 00:08:00  \n",
       "5612802   20994577573871616 2011-01-01 00:07:52  \n",
       "5612803   20993318066327552 2011-01-01 00:02:51  \n",
       "\n",
       "[4868804 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter['created_at'] = pd.to_datetime(twitter['created_at']).dt.tz_localize(None)\n",
    "twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83205ce3-88bc-43ac-a0fd-f1a68ce48189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with removing all users where username contains 'bot' or 'quake'\n",
    "remove = ['quake', 'bot']\n",
    "dw = twitter[~df.username.str.contains('|'.join(remove), case=False, na=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23c8ebaa-e86c-45c1-b8f5-4a67f3b55093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove users which posted more tweets than certain threshhold\n",
    "post_counts = twitter.author_id.value_counts()\n",
    "lst_id = post_counts.index.tolist()\n",
    "lst_count = post_counts.tolist()\n",
    "\n",
    "post_counts = pd.DataFrame(lst_id).rename(columns={0: 'author_id'})\n",
    "post_counts['value'] = lst_count\n",
    "\n",
    "\n",
    "bots = post_counts.loc[post_counts['value'] > 100].sort_values(by=['value'])\n",
    "bots = bots.author_id.tolist()\n",
    "\n",
    "#dw[~dw.username.str.contains('|'.join(bots), case=False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5976da9-241d-4b81-a533-435c2961a61c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "twitter['author_id'] = twitter['author_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b60360e-4300-4f49-8ae9-a0f201e5c791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#2011memories the earthquake when i was at @th...</td>\n",
       "      <td>152378466</td>\n",
       "      <td>153264156865339395</td>\n",
       "      <td>2011-12-31 23:59:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I think the earthquake hurt my stuffed animals...</td>\n",
       "      <td>30585588</td>\n",
       "      <td>153263967727394816</td>\n",
       "      <td>2011-12-31 23:59:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#YourNewsTweet -  2nd quake in a week rattles ...</td>\n",
       "      <td>380125872</td>\n",
       "      <td>153263927428530176</td>\n",
       "      <td>2011-12-31 23:59:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>that earthquake woke me up to a new life</td>\n",
       "      <td>324587833</td>\n",
       "      <td>153263924681252864</td>\n",
       "      <td>2011-12-31 23:59:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The pursuit of: The Earthquake http://t.co/N4z...</td>\n",
       "      <td>54406011</td>\n",
       "      <td>153263889432317952</td>\n",
       "      <td>2011-12-31 23:58:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5612797</th>\n",
       "      <td>Top 10 \"Seriously Good News\" From 2010:  2010 ...</td>\n",
       "      <td>17855061</td>\n",
       "      <td>20995413997789185</td>\n",
       "      <td>2011-01-01 00:11:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5612798</th>\n",
       "      <td>OK. Either someone's been selling the wrong so...</td>\n",
       "      <td>67354453</td>\n",
       "      <td>20995164642222080</td>\n",
       "      <td>2011-01-01 00:10:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5612800</th>\n",
       "      <td>waiting for the earthquake like @jeiheel</td>\n",
       "      <td>56163369</td>\n",
       "      <td>20994734549901313</td>\n",
       "      <td>2011-01-01 00:08:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5612801</th>\n",
       "      <td>US scientists warned in 2008 that Port-au-Prin...</td>\n",
       "      <td>26184071</td>\n",
       "      <td>20994614701850625</td>\n",
       "      <td>2011-01-01 00:08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5612802</th>\n",
       "      <td>#2010Memories traveling to Haiti after earthqu...</td>\n",
       "      <td>20287993</td>\n",
       "      <td>20994577573871616</td>\n",
       "      <td>2011-01-01 00:07:52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3710097 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  author_id  \\\n",
       "0        #2011memories the earthquake when i was at @th...  152378466   \n",
       "2        I think the earthquake hurt my stuffed animals...   30585588   \n",
       "3        #YourNewsTweet -  2nd quake in a week rattles ...  380125872   \n",
       "4                 that earthquake woke me up to a new life  324587833   \n",
       "5        The pursuit of: The Earthquake http://t.co/N4z...   54406011   \n",
       "...                                                    ...        ...   \n",
       "5612797  Top 10 \"Seriously Good News\" From 2010:  2010 ...   17855061   \n",
       "5612798  OK. Either someone's been selling the wrong so...   67354453   \n",
       "5612800           waiting for the earthquake like @jeiheel   56163369   \n",
       "5612801  US scientists warned in 2008 that Port-au-Prin...   26184071   \n",
       "5612802  #2010Memories traveling to Haiti after earthqu...   20287993   \n",
       "\n",
       "                         id          created_at  \n",
       "0        153264156865339395 2011-12-31 23:59:57  \n",
       "2        153263967727394816 2011-12-31 23:59:12  \n",
       "3        153263927428530176 2011-12-31 23:59:02  \n",
       "4        153263924681252864 2011-12-31 23:59:01  \n",
       "5        153263889432317952 2011-12-31 23:58:53  \n",
       "...                     ...                 ...  \n",
       "5612797   20995413997789185 2011-01-01 00:11:11  \n",
       "5612798   20995164642222080 2011-01-01 00:10:11  \n",
       "5612800   20994734549901313 2011-01-01 00:08:29  \n",
       "5612801   20994614701850625 2011-01-01 00:08:00  \n",
       "5612802   20994577573871616 2011-01-01 00:07:52  \n",
       "\n",
       "[3710097 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25d4660a-6e9b-4a1e-8b32-b0384b070aa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = twitter[~twitter.author_id.isin(bots)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aba7ecc-da93-4664-a6ec-5bd13999fd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql('twitter2011', con=engine, if_exists='append', chunksize=1000, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "67d47e51-6d59-4c83-a60d-043cfdbc4fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dw.to_sql('dw_twitter', con=engine, if_exists='append', chunksize=1000, index=False)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f92614-ede1-4042-a143-790a27a773d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out accounts with more or equal to 175 postings\n",
    "df = df.loc[df['value'] > 10].sort_values(by=['value'])\n",
    "lst_bots = df_a.username.tolist()\n",
    "\n",
    "\n",
    "\n",
    "# add -from: operator to each entry in list \n",
    "append_str = '-from:'\n",
    "lst_filter = [append_str + sub for sub in lst_bots]\n",
    "\n",
    "# convert list to a string suitable for the query\n",
    "filter_names_query = ' '.join(lst_filter)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
